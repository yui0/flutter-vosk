import 'dart:io';
import 'package:flutter/material.dart';
import 'package:path_provider/path_provider.dart';
import 'package:vosk_flutter/vosk_flutter.dart';
import 'package:record/record.dart';

void main() {
  runApp(const MyApp());
}

class MyApp extends StatelessWidget {
  const MyApp({Key? key}) : super(key: key);

  @override
  Widget build(BuildContext context) {
    return const MaterialApp(
      home: KotoriMemo(),
    );
  }
}

class KotoriMemo extends StatefulWidget {
  const KotoriMemo({Key? key}) : super(key: key);

  @override
  State<KotoriMemo> createState() => _KotoriMemoState();
}

class _KotoriMemoState extends State<KotoriMemo> {
  static const _textStyle = TextStyle(fontSize: 20, color: Colors.black);
  static const _modelName = 'vosk-model-small-en-us-0.15';
  static const _sampleRate = 16000;

  final _vosk = VoskFlutterPlugin.instance();
  final _modelLoader = ModelLoader();
  //final _recorder = Record();

  String? _fileRecognitionResult;
  String? _error;
  Model? _model;
  Recognizer? _recognizer;
  SpeechService? _speechService;

  bool _recognitionStarted = false;

  @override
  void initState() {
    super.initState();
    _initializeVosk();
  }

  Future<void> _initializeVosk() async {
    try {
      final Directory tempDir = await getTemporaryDirectory();
      final Directory appDocumentsDir = await getApplicationDocumentsDirectory();
      final Directory? downloadsDir = await getDownloadsDirectory();
      print(tempDir);
      print(appDocumentsDir);
      print(downloadsDir);

      final modelsList = await _modelLoader.loadModelsList();
      final modelDescription = modelsList.firstWhere(
        (model) => model.name == _modelName,
        orElse: () => throw Exception('Model $_modelName not found'),
      );
      final modelPath = await _modelLoader.loadFromNetwork(modelDescription.url);
      final model = await _vosk.createModel(modelPath);

      setState(() => _model = model);
      _recognizer = await _vosk.createRecognizer(model: _model!, sampleRate: _sampleRate);

      if (Platform.isAndroid) {
        _speechService = await _vosk.initSpeechService(_recognizer!);
      }
    } catch (e, stacktrace) {
      setState(() => _error = e.toString());
      print('Exception: '+e.toString());
      print('Stacktrace: '+stacktrace.toString());
    }
  }

  @override
  Widget build(BuildContext context) {
    if (_error != null) {
      return Scaffold(
        body: Center(
          child: Text("[Error] $_error", style: _textStyle),
        ),
      );
    } else if (_model == null) {
      return const Scaffold(
        body: Center(
          child: Text("Loading model...", style: _textStyle),
        ),
      );
    } else if (Platform.isAndroid && _speechService == null) {
      return const Scaffold(
        body: Center(
          child: Text("Initializing speech service...", style: _textStyle),
        ),
      );
    } else {
      return Platform.isAndroid ? _androidExample() : _commonExample();
    }
  }

  Widget _androidExample() {
    return Scaffold(
      body: Center(
        child: Column(
          mainAxisAlignment: MainAxisAlignment.center,
          children: [
            ElevatedButton(
              onPressed: _toggleRecording,
              child: Text(
                _recognitionStarted ? "Stop recognition" : "Start recognition",
              ),
            ),
            StreamBuilder(
              stream: _speechService!.onPartial(),
              builder: (context, snapshot) => Text(
                "Partial result: ${snapshot.data ?? ""}",
                style: _textStyle,
              ),
            ),
            StreamBuilder(
              stream: _speechService!.onResult(),
              builder: (context, snapshot) => Text(
                "Result: ${snapshot.data ?? ""}",
                style: _textStyle,
              ),
            ),
          ],
        ),
      ),
    );
  }

  Widget _commonExample() {
    return Scaffold(
      body: Center(
        child: Column(
          mainAxisAlignment: MainAxisAlignment.center,
          children: [
            ElevatedButton(
              onPressed: _toggleRecording,
              child: Text(
                _recognitionStarted ? "Stop recording" : "Record audio",
              ),
            ),
            Text(
              "Final recognition result: $_fileRecognitionResult",
              style: _textStyle,
            ),
          ],
        ),
      ),
    );
  }

  Future<void> _toggleRecording() async {
    if (_recognitionStarted) {
      if (Platform.isAndroid) {
        await _speechService?.stop();
      } else {
        await _stopRecording();
      }
    } else {
      if (Platform.isAndroid) {
        await _speechService?.start();
      } else {
        await _recordAudio();
      }
    }
    setState(() => _recognitionStarted = !_recognitionStarted);
  }

  Future<void> _recordAudio() async {
    /*try {
      await _recorder.start(
        samplingRate: _sampleRate,
        encoder: AudioEncoder.wav,
        numChannels: 1,
      );
    } catch (e) {
      setState(() => _error = "Recording error: $e");
    }*/
  }

  Future<void> _stopRecording() async {
    /*try {
      final filePath = await _recorder.stop();
      if (filePath != null) {
        final bytes = File(filePath).readAsBytesSync();
        _recognizer?.acceptWaveformBytes(bytes);
        final result = await _recognizer?.getFinalResult();
        setState(() => _fileRecognitionResult = result);
      }
    } catch (e) {
      setState(() => _error = "Recognition error: $e");
    }*/
  }
}
